{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "#TODO: read tf documentation on what the static_rnn does\n",
    "#do I want to one hot encode the data? or already too many dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Input, Dense, GRU, Embedding, LSTM\n",
    "from tensorflow.python.keras.optimizers import RMSprop\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.12.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open files, they have not been normalized yet\n",
    "dffile = open('masdfr4m7170317s1.pkl',\"rb\")\n",
    "dyfile = open('r4m7170317s1.pkl', \"rb\")\n",
    "df = pickle.load(dffile)\n",
    "dy = pickle.load(dyfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      intensity  pupilPeri  wheelPeri  responseT  whiskPeri  lastStimT  \\\n",
      "0           1.0  28.572136  20.540612   0.417911  18.868394        0.0   \n",
      "1           1.0  28.502843  22.147099   0.417911  16.260633        0.1   \n",
      "2           1.0  28.397470  20.402827   0.417911  13.750951        0.2   \n",
      "3           1.0  28.275008  17.837755   0.417911  13.613636        0.3   \n",
      "4           1.0  28.259277  10.929228   0.417911  15.728795        0.4   \n",
      "5           1.0  28.411358   6.836825   0.417911  18.009669        0.5   \n",
      "6           1.0  28.727576   2.964386   0.417911  18.112827        0.6   \n",
      "7           1.0  28.953888   5.695584   0.417911  17.157140        0.7   \n",
      "8           1.0  28.969450   8.156320   0.417911  16.636804        0.8   \n",
      "9           1.0  28.840834  10.485815   0.417911  16.797964        0.9   \n",
      "10          1.0  28.814819  10.614280   0.417911  16.164776        1.0   \n",
      "11          1.0  28.817211   9.009948   0.417911  13.742710        1.1   \n",
      "12          1.0  28.919805   7.288404   0.417911  11.502438        1.2   \n",
      "13          1.0  29.051138   7.870120   0.417911  13.464179        1.3   \n",
      "14          1.0  29.242827   9.028000   0.417911  15.927727        1.4   \n",
      "15          1.0  29.266881   9.009341   0.417911  18.806123        1.5   \n",
      "16          1.0  29.080680   5.848081   0.417911  19.518351        1.6   \n",
      "17          1.0  28.842263   4.552090   0.417911  19.396033        1.7   \n",
      "18          1.0  28.718631   5.300118   0.417911  18.719340        1.8   \n",
      "19          1.0  28.813660   7.536169   0.417911  18.952262        1.9   \n",
      "20          1.0  24.376780   0.090879   0.438027  22.129406        0.0   \n",
      "21          1.0  24.245880   0.216180   0.438027  20.473895        0.1   \n",
      "22          1.0  24.210022   0.293068   0.438027  22.786308        0.2   \n",
      "23          1.0  24.114249   0.446525   0.438027  22.643297        0.3   \n",
      "24          1.0  24.333509   0.738097   0.438027  24.098389        0.4   \n",
      "25          1.0  24.664155   0.682276   0.438027  25.410268        0.5   \n",
      "26          1.0  25.269861   2.369275   0.438027  21.984492        0.6   \n",
      "27          1.0  25.849253   5.935875   0.438027  21.211765        0.7   \n",
      "28          1.0  26.562911  10.000345   0.438027  17.202250        0.8   \n",
      "29          1.0  27.043258  11.277648   0.438027  16.436980        0.9   \n",
      "...         ...        ...        ...        ...        ...        ...   \n",
      "2370       -1.0  20.354405   0.027851   0.000000   6.313270        1.0   \n",
      "2371       -1.0  20.084835   0.035010   0.000000   5.549843        1.1   \n",
      "2372       -1.0  19.958280   0.035010   0.000000   4.296977        1.2   \n",
      "2373       -1.0  19.691773   0.035010   0.000000   6.534263        1.3   \n",
      "2374       -1.0  19.514659   0.815906   0.000000  10.571333        1.4   \n",
      "2375       -1.0  19.321383   2.233063   0.000000  15.322511        1.5   \n",
      "2376       -1.0  19.243549   2.240176   0.000000  19.564721        1.6   \n",
      "2377       -1.0  19.319597   2.377793   0.000000  20.179202        1.7   \n",
      "2378       -1.0  19.382107   1.023500   0.000000  21.614612        1.8   \n",
      "2379       -1.0  19.347455   1.428477   0.000000  20.545383        1.9   \n",
      "2380        1.0  18.215443   0.028027   0.361477   4.547274        0.0   \n",
      "2381        1.0  18.289827   0.028027   0.361477   5.530089        0.1   \n",
      "2382        1.0  18.263384   0.028027   0.361477   4.823415        0.2   \n",
      "2383        1.0  18.224231   0.027830   0.361477   4.642072        0.3   \n",
      "2384        1.0  18.121561   0.027830   0.361477   5.680010        0.4   \n",
      "2385        1.0  18.207856   0.027830   0.361477  11.843632        0.5   \n",
      "2386        1.0  18.312569   0.716631   0.361477  17.794563        0.6   \n",
      "2387        1.0  18.565299   2.459502   0.361477  20.536071        0.7   \n",
      "2388        1.0  18.849164   3.486928   0.361477  17.430712        0.8   \n",
      "2389        1.0  19.388370   4.292810   0.361477  15.046406        0.9   \n",
      "2390        1.0  19.888677   5.312842   0.361477  14.403109        1.0   \n",
      "2391        1.0  20.425317   7.867792   0.361477  14.990117        1.1   \n",
      "2392        1.0  20.754644  11.039690   0.361477  16.837448        1.2   \n",
      "2393        1.0  21.051926  13.230864   0.361477  19.199111        1.3   \n",
      "2394        1.0  21.079746  13.865796   0.361477  20.763320        1.4   \n",
      "2395        1.0  21.043080  12.025091   0.361477  21.760133        1.5   \n",
      "2396        1.0  20.930711   7.161475   0.361477  19.820906        1.6   \n",
      "2397        1.0  20.969298   4.124739   0.361477  20.485907        1.7   \n",
      "2398        1.0  21.031927   1.278127   0.361477  19.044420        1.8   \n",
      "2399        1.0  20.981655   1.487510   0.361477  21.824116        1.9   \n",
      "\n",
      "      pupilMovement    vidPeri  \n",
      "0          0.217349  10.833909  \n",
      "1          0.222630  10.007257  \n",
      "2          0.200246   8.939611  \n",
      "3          0.242772   8.272795  \n",
      "4          0.256109   8.702853  \n",
      "5          0.287546   9.768923  \n",
      "6          0.238537  11.154133  \n",
      "7          0.764740  12.202072  \n",
      "8          1.126251  13.224731  \n",
      "9          2.138323  14.294003  \n",
      "10         1.796792  13.723986  \n",
      "11         1.526484  10.932270  \n",
      "12         1.016848   8.360343  \n",
      "13         0.853256   9.303009  \n",
      "14         0.807510  10.246297  \n",
      "15         1.122181  12.054803  \n",
      "16         1.689204  12.109601  \n",
      "17         1.772347  13.245364  \n",
      "18         2.187235  13.836007  \n",
      "19         2.785126  14.268391  \n",
      "20         0.116872  11.673187  \n",
      "21         0.207363  11.127259  \n",
      "22         0.260585  12.314284  \n",
      "23         0.334635  12.382544  \n",
      "24         0.338051  13.105277  \n",
      "25         0.392338  14.632632  \n",
      "26         0.727734  14.453263  \n",
      "27         0.851576  14.707661  \n",
      "28         1.135145  12.756284  \n",
      "29         1.129217  11.953073  \n",
      "...             ...        ...  \n",
      "2370       0.144947   3.728757  \n",
      "2371       0.209991   3.548083  \n",
      "2372       0.265471   2.652411  \n",
      "2373       0.292363   4.246596  \n",
      "2374       0.446354   7.330161  \n",
      "2375       0.389802  10.491446  \n",
      "2376       0.436030  12.812579  \n",
      "2377       0.336010  12.312268  \n",
      "2378       0.370644  12.288628  \n",
      "2379       0.319284  11.733586  \n",
      "2380       0.021893   2.170263  \n",
      "2381       0.073711   2.541079  \n",
      "2382       0.102837   2.290477  \n",
      "2383       0.118757   2.222774  \n",
      "2384       0.085247   2.606088  \n",
      "2385       0.212443   6.606255  \n",
      "2386       0.595676  11.454730  \n",
      "2387       0.706249  14.072651  \n",
      "2388       0.921400  12.580123  \n",
      "2389       0.668040  10.600768  \n",
      "2390       0.882630  10.482840  \n",
      "2391       0.614937  10.858586  \n",
      "2392       0.534221  11.300790  \n",
      "2393       0.533125  11.919137  \n",
      "2394       0.627560  13.285029  \n",
      "2395       0.663030  13.913896  \n",
      "2396       0.375923  13.119974  \n",
      "2397       0.211006  13.104517  \n",
      "2398       0.156712  12.378861  \n",
      "2399       0.289416  13.955863  \n",
      "\n",
      "[2400 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df) # do I want to take out lastStimT ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "Shape: (2400, 8)\n"
     ]
    }
   ],
   "source": [
    "x_data = df.values\n",
    "print(type(x_data))\n",
    "print(\"Shape:\", x_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "Shape: (2400, 22360)\n"
     ]
    }
   ],
   "source": [
    "print(type(dy))\n",
    "print(\"Shape:\", dy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400\n",
      "2160\n",
      "240\n"
     ]
    }
   ],
   "source": [
    "num_data = len(x_data)\n",
    "print(num_data)\n",
    "train_split = 0.9\n",
    "num_train = int(train_split*num_data)\n",
    "print(num_train)\n",
    "num_test = num_data - num_train\n",
    "print(num_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2400"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = x_data[0:num_train]\n",
    "x_test = x_data[num_train:]\n",
    "len(x_train) + len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2400"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = dy[0:num_train]\n",
    "y_test = dy[num_train:]\n",
    "len(y_train) + len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22360"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_x_signals = x_data.shape[1]\n",
    "num_x_signals\n",
    "num_y_signals = dy.shape[1]\n",
    "num_y_signals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: -1.0\n",
      "Max: 29.2668806589132\n"
     ]
    }
   ],
   "source": [
    "print(\"Min:\", np.min(x_train))\n",
    "print(\"Max:\", np.max(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_scaler = MinMaxScaler()\n",
    "x_train_scaled = x_scaler.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 0.0\n",
      "Max: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Min:\", np.min(x_train_scaled))\n",
    "print(\"Max:\", np.max(x_train_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_scaled = x_scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scaler = MinMaxScaler()\n",
    "y_train_scaled = y_scaler.fit_transform(y_train)\n",
    "y_test_scaled = y_scaler.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generator\n",
    "useful for large amounts of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(batch_size, sequence_length):\n",
    "    \"\"\"\n",
    "    Generator function for creating random batches of training-data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Infinite loop.\n",
    "    while True:\n",
    "        # Allocate a new array for the batch of input-signals.\n",
    "        x_shape = (batch_size, sequence_length, num_x_signals)\n",
    "        x_batch = np.zeros(shape=x_shape, dtype=np.float16)\n",
    "\n",
    "        # Allocate a new array for the batch of output-signals.\n",
    "        y_shape = (batch_size, sequence_length, num_y_signals)\n",
    "        y_batch = np.zeros(shape=y_shape, dtype=np.float16)\n",
    "\n",
    "        # Fill the batch with random sequences of data.\n",
    "        for i in range(batch_size):\n",
    "            # Get a random start-index.\n",
    "            # This points somewhere into the training-data.\n",
    "            idx = np.random.randint(num_train - sequence_length)\n",
    "            \n",
    "            # Copy the sequences of data starting at this index.\n",
    "            x_batch[i] = x_train_scaled[idx:idx+sequence_length]\n",
    "            y_batch[i] = y_train_scaled[idx:idx+sequence_length]\n",
    "        \n",
    "        yield (x_batch, y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 200\n",
    "sequence_length = 0 #20 frames after stim time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = batch_generator(batch_size=batch_size, sequence_length=sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_batch, y_batch = next(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 20, 8)\n",
      "(40, 20, 22360)\n"
     ]
    }
   ],
   "source": [
    "print(x_batch.shape)\n",
    "print(y_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = (np.expand_dims(x_test_scaled, axis=0),\n",
    "                   np.expand_dims(y_test_scaled, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32,\n",
    "              activation='sigmoid',\n",
    "              input_shape=(None, num_x_signals,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# squash the outputs to be between 0 and 1\n",
    "model.add(Dense(num_y_signals, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A problem with using the Sigmoid activation function, is that we can now only output values in the same range as the training-data.\n",
    "\n",
    "For example, if the training-data only has temperatures between -20 and +30 degrees, then the scaler-object will map -20 to 0 and +30 to 1. So if we limit the output of the neural network to be between 0 and 1 using the Sigmoid function, this can only be mapped back to temperature values between -20 and +30.\n",
    "\n",
    "We can use a linear activation function on the output instead. This allows for the output to take on arbitrary values. It might work with the standard initialization for a simple network architecture, but for more complicated network architectures e.g. with more layers, it might be necessary to initialize the weights with smaller values to avoid NaN values during training. You may need to experiment with this to get it working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    from tensorflow.python.keras.initializers import RandomUniform\n",
    "\n",
    "    # Maybe use lower init-ranges.\n",
    "    init = RandomUniform(minval=-0.05, maxval=0.05)\n",
    "\n",
    "    model.add(Dense(num_y_signals,\n",
    "                    activation='linear',\n",
    "                    kernel_initializer=init))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "warmup_steps = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_mse_warmup(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the Mean Squared Error between y_true and y_pred,\n",
    "    but ignore the beginning \"warmup\" part of the sequences.\n",
    "    \n",
    "    y_true is the desired output.\n",
    "    y_pred is the model's output.\n",
    "    \"\"\"\n",
    "\n",
    "    # The shape of both input tensors are:\n",
    "    # [batch_size, sequence_length, num_y_signals].\n",
    "\n",
    "    # Ignore the \"warmup\" parts of the sequences\n",
    "    # by taking slices of the tensors.\n",
    "    y_true_slice = y_true[:, warmup_steps:, :]\n",
    "    y_pred_slice = y_pred[:, warmup_steps:, :]\n",
    "\n",
    "    # These sliced tensors both have this shape:\n",
    "    # [batch_size, sequence_length - warmup_steps, num_y_signals]\n",
    "\n",
    "    # Calculate the MSE loss for each value in these tensors.\n",
    "    # This outputs a 3-rank tensor of the same shape.\n",
    "    loss = tf.losses.mean_squared_error(labels=y_true_slice,\n",
    "                                        predictions=y_pred_slice)\n",
    "\n",
    "    # Keras may reduce this across the first axis (the batch)\n",
    "    # but the semantics are unclear, so to be sure we use\n",
    "    # the loss across the entire tensor, we reduce it to a\n",
    "    # single scalar with the mean function.\n",
    "    loss_mean = tf.reduce_mean(loss)\n",
    "\n",
    "    return loss_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = RMSprop(lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, None, 32)          288       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, None, 22360)       737880    \n",
      "=================================================================\n",
      "Total params: 738,168\n",
      "Trainable params: 738,168\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_checkpoint = '23_checkpoint.keras'\n",
    "callback_checkpoint = ModelCheckpoint(filepath=path_checkpoint,\n",
    "                                      monitor='val_loss',\n",
    "                                      verbose=1,\n",
    "                                      save_weights_only=True,\n",
    "                                      save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                                        patience=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_tensorboard = TensorBoard(log_dir='./23_logs/',\n",
    "                                   histogram_freq=0,\n",
    "                                   write_graph=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                       factor=0.1,\n",
    "                                       min_lr=1e-4,\n",
    "                                       patience=0,\n",
    "                                       verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [callback_early_stopping,\n",
    "             callback_checkpoint,\n",
    "             callback_tensorboard,\n",
    "             callback_reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0293\n",
      "Epoch 00001: val_loss improved from inf to 0.02932, saving model to 23_checkpoint.keras\n",
      "100/100 [==============================] - 23s 229ms/step - loss: 0.0294 - val_loss: 0.0293\n",
      "Epoch 2/20\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0260\n",
      "Epoch 00002: val_loss improved from 0.02932 to 0.02764, saving model to 23_checkpoint.keras\n",
      "100/100 [==============================] - 19s 189ms/step - loss: 0.0259 - val_loss: 0.0276\n",
      "Epoch 3/20\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0231\n",
      "Epoch 00003: val_loss improved from 0.02764 to 0.02483, saving model to 23_checkpoint.keras\n",
      "100/100 [==============================] - 19s 194ms/step - loss: 0.0231 - val_loss: 0.0248\n",
      "Epoch 4/20\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0200\n",
      "Epoch 00004: val_loss improved from 0.02483 to 0.02330, saving model to 23_checkpoint.keras\n",
      "100/100 [==============================] - 20s 196ms/step - loss: 0.0200 - val_loss: 0.0233\n",
      "Epoch 5/20\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0181\n",
      "Epoch 00005: val_loss improved from 0.02330 to 0.02240, saving model to 23_checkpoint.keras\n",
      "100/100 [==============================] - 19s 188ms/step - loss: 0.0181 - val_loss: 0.0224\n",
      "Epoch 6/20\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0171\n",
      "Epoch 00006: val_loss did not improve from 0.02240\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "100/100 [==============================] - 18s 183ms/step - loss: 0.0171 - val_loss: 0.0239\n",
      "Epoch 7/20\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00007: val_loss did not improve from 0.02240\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
      "100/100 [==============================] - 18s 183ms/step - loss: 0.0167 - val_loss: 0.0231\n",
      "Epoch 8/20\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00008: val_loss did not improve from 0.02240\n",
      "100/100 [==============================] - 18s 183ms/step - loss: 0.0166 - val_loss: 0.0230\n",
      "Epoch 9/20\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00009: val_loss did not improve from 0.02240\n",
      "100/100 [==============================] - 19s 188ms/step - loss: 0.0167 - val_loss: 0.0231\n",
      "Epoch 10/20\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00010: val_loss did not improve from 0.02240\n",
      "100/100 [==============================] - 18s 182ms/step - loss: 0.0168 - val_loss: 0.0235\n",
      "Epoch 00010: early stopping\n",
      "CPU times: user 3min 27s, sys: 1min 49s, total: 5min 16s\n",
      "Wall time: 3min 12s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa9fa849a20>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit_generator(generator=generator,\n",
    "                    epochs=20,\n",
    "                    steps_per_epoch=100,\n",
    "                    validation_data=validation_data,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-aa335f809cc8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m result = model.evaluate(x=np.expand_dims(x_test_scaled, axis=0),\n\u001b[0m\u001b[1;32m      2\u001b[0m                         y=np.expand_dims(y_test_scaled, axis=0))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(x=np.expand_dims(x_test_scaled, axis=0),\n",
    "                        y=np.expand_dims(y_test_scaled, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-91ddf97231a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loss (test-set):\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"loss (test-set):\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparison(start_idx, length=100, train=True):\n",
    "    \"\"\"\n",
    "    Plot the predicted and true output-signals.\n",
    "    \n",
    "    :param start_idx: Start-index for the time-series.\n",
    "    :param length: Sequence-length to process and plot.\n",
    "    :param train: Boolean whether to use training- or test-set.\n",
    "    \"\"\"\n",
    "    \n",
    "    if train:\n",
    "        # Use training-data.\n",
    "        x = x_train_scaled\n",
    "        y_true = y_train\n",
    "    else:\n",
    "        # Use test-data.\n",
    "        x = x_test_scaled\n",
    "        y_true = y_test\n",
    "    \n",
    "    # End-index for the sequences.\n",
    "    end_idx = start_idx + length\n",
    "    \n",
    "    # Select the sequences from the given start-index and\n",
    "    # of the given length.\n",
    "    x = x[start_idx:end_idx]\n",
    "    y_true = y_true[start_idx:end_idx]\n",
    "    \n",
    "    # Input-signals for the model.\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "\n",
    "    # Use the model to predict the output-signals.\n",
    "    y_pred = model.predict(x)\n",
    "    print(y_pred)\n",
    "    print(len(y_pred[0]))\n",
    "    print(len(y_pred[0][0]))\n",
    "    \n",
    "    # The output of the model is between 0 and 1.\n",
    "    # Do an inverse map to get it back to the scale\n",
    "    # of the original data-set.\n",
    "    y_pred_rescaled = y_scaler.inverse_transform(y_pred[0])\n",
    "    print(y_pred_rescaled)\n",
    "    print(len(y_pred_rescaled))\n",
    "    print(len(y_pred_rescaled[0]))\n",
    "    # For each output-signal.\n",
    "    for signal in range(1):\n",
    "        # Get the output-signal predicted by the model.\n",
    "        signal_pred = np.sum(np.abs(y_pred_rescaled), axis=0)\n",
    "        print(signal_pred)\n",
    "        # Get the true output-signal from the data-set.\n",
    "        signal_true = np.sum(np.abs(y_true), axis=0)\n",
    "        print(len(signal_true[400:800]))\n",
    "        print(signal_true)\n",
    "        # Make the plotting-canvas bigger.\n",
    "        plt.figure(figsize=(15,5))\n",
    "        \n",
    "        # Plot and compare the two signals.\n",
    "        plt.plot(signal_true[100:800], label='true')\n",
    "        plt.plot(signal_pred[100:800], label='pred')\n",
    "        \n",
    "        # Plot grey box for warmup-period.\n",
    "        p = plt.axvspan(0, warmup_steps, facecolor='black', alpha=0.15)\n",
    "        \n",
    "        # Plot labels etc.\n",
    "        plt.ylabel(\"sum\")\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train_scaled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-8bec304b249c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_comparison\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-80ce9decaaa0>\u001b[0m in \u001b[0;36mplot_comparison\u001b[0;34m(start_idx, length, train)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# Use training-data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train_scaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_train_scaled' is not defined"
     ]
    }
   ],
   "source": [
    "plot_comparison(start_idx=1, length=1, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
